# Learning Rate Annealing
余弦退火学习率是一种改进深度神经网络学习过程的常用方法。当深度神经网络在大型数据集上训练时，学习过程可能会陷入局部极小值。余弦退火学习率通过在训练过程中逐渐降低学习率，帮助模型更好地收敛，从而提高预测的准确性。

- 阶段：现在大家预训练分成三个阶段：快速收敛阶段，稳定阶段，退火阶段。
- LR scheduler：决定learning rate的大小，与优化器协同工作。
- 退火加sft &“和面”：在退火阶段使用高质量数据，可以提升学习效率和模型对benchmark的认识。
- 再看scaling law：随着数据、训练和参数的增长，loss会不断下降。https://news.miracleplus.com/share_link/20953  

模擬退火（英語：Simulated annealing，縮寫作SA）

https://huaweicloud.csdn.net/6380308adacf622b8df8694e.html?dp_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpZCI6MjE3NzgwNSwiZXhwIjoxNzIzNjU1OTQ3LCJpYXQiOjE3MjMwNTExNDcsInVzZXJuYW1lIjoieWltdmgifQ.ESHtRY43W9QLwVVV6QGtpJnpngc-ta1fvetprMsiKSI  

# Supervised fine-tuning (SFT) in LMM
Supervised fine-tuning (SFT) is a powerful technique used to adapt pre-trained Large Language Models (LLMs) to specific downstream tasks using labeled data.  
https://klu.ai/glossary/supervised-fine-tuning  

What are some common supervised fine-tuning techniques?

LoRA (Low-Rank Adaptation) — A parameter-efficient fine-tuning technique that uses low-rank decomposition to represent weight updates with two smaller matrices, reducing the number of trainable parameters.

QLoRA (Quantized LoRA) — A memory-efficient variant of LoRA that further reduces the memory requirements for fine-tuning large LLMs.

# flex for Black Forest Labs
BFL comes from SD  
https://blackforestlabs.ai/announcements/  
https://github.com/black-forest-labs/flux  
text to image text to video

