# Learning Rate Annealing
余弦退火学习率是一种改进深度神经网络学习过程的常用方法。当深度神经网络在大型数据集上训练时，学习过程可能会陷入局部极小值。余弦退火学习率通过在训练过程中逐渐降低学习率，帮助模型更好地收敛，从而提高预测的准确性。

- 阶段：现在大家预训练分成三个阶段：快速收敛阶段，稳定阶段，退火阶段。
- LR scheduler：决定learning rate的大小，与优化器协同工作。
- 退火加sft &“和面”：在退火阶段使用高质量数据，可以提升学习效率和模型对benchmark的认识。
- 再看scaling law：随着数据、训练和参数的增长，loss会不断下降。https://news.miracleplus.com/share_link/20953  

模擬退火（英語：Simulated annealing，縮寫作SA）

https://huaweicloud.csdn.net/6380308adacf622b8df8694e.html?dp_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpZCI6MjE3NzgwNSwiZXhwIjoxNzIzNjU1OTQ3LCJpYXQiOjE3MjMwNTExNDcsInVzZXJuYW1lIjoieWltdmgifQ.ESHtRY43W9QLwVVV6QGtpJnpngc-ta1fvetprMsiKSI  

# SFT->DPO/PPO
## Supervised fine-tuning (SFT) in LMM
Supervised fine-tuning (SFT) is a powerful technique used to adapt pre-trained Large Language Models (LLMs) to specific downstream tasks using labeled data.  
https://klu.ai/glossary/supervised-fine-tuning  

What are some common supervised fine-tuning techniques?


## Lora

LoRA (Low-Rank Adaptation) — A parameter-efficient fine-tuning technique that uses low-rank decomposition to represent weight updates with two smaller matrices, reducing the number of trainable parameters.

QLoRA (Quantized LoRA) — A memory-efficient variant of LoRA that further reduces the memory requirements for fine-tuning large LLMs.


## QLora

## Direct Preference Optimization (DPO)



## PPO

# flex for Black Forest Labs
BFL comes from SD  
https://blackforestlabs.ai/announcements/  
https://github.com/black-forest-labs/flux  
text to image text to video

# Mate  SAM 2: Segment Anything in Images and Videos
The basic segment tool https://github.com/facebookresearch/segment-anything-2

# Power BI
Microsoft Power BI  
For beginner https://www.youtube.com/watch?v=TmhQCQr_DCA&t=1500s

# Extract, Load, Transform (ELT)  
Extract, Load, Transform (ELT) is a data integration process for transferring raw data from a source server to a data system (such as a data warehouse or data lake) on a target server and then preparing the information for downstream uses.  
https://www.ibm.com/topics/elt  
https://www.techtarget.com/searchdatamanagement/definition/Extract-Load-Transform-ELT


# Measure  one  distribution diverges from  second

## MinHash  
MinHash is a technique for estimating the similarity between two sets.  
https://spotintelligence.com/2023/01/02/minhash/#:~:text=MinHash%20calculates%20the%20similarity%20between,likely%20the%20groups%20are%20identical.


https://blog.csdn.net/Ning_yuan/article/details/126907690#:~:text=MinHash%EF%BC%88%E6%9C%80%E5%B0%8F%E5%93%88%E5%B8%8C%EF%BC%89%E6%98%AF,%E7%9B%B8%E4%BC%BC%E6%80%A7%E6%A3%80%E6%B5%8B%E7%AD%89%E5%9C%BA%E6%99%AF%E3%80%82

Shingling:将文本文档转换为集合表示 (通常是转换为布尔型向量）  
Min-Hashing: 将高维度的向量转换为低维的哈希签名，此时再计算哈希签名的相似性  
Locality-Sensitive Hashing: 重点关注来自相似文档的一对候选哈希签名  

## KL divergence

Distribution P and Q

![alt text](image-1.png)
In summary, the KL divergence between reflects how well the distribution approximates the true distribution

True vs. Approximate Distribution

# What is RAG (Retrieval-Augmented Generation)?  

检索增强生成（RAG）是一个概念，它旨在为大语言模型（LLM）提供额外的、来自外部知识源的信息。这样，LLM 在生成更精确、更贴合上下文的答案的同时，也能有效减少产生误导性信息的可能。


LLM 是用于生成富有创意且富有吸引力的文本的强大工具，但它们有时难以确保事实的准确性。这是因为 LLM 是使用大量文本数据训练的，其中可能包含不准确的信息或存在偏差的信息。

RAG 通过为 LLM 提供精选知识库来帮助解决这一问题，确保生成的文本以事实信息为基础。这使得 RAG 对准确性至关重要的应用（例如新闻报道、科学写作或客户服务）特别有价值。

注意：RAG 还可以帮助防止将幻觉发送给最终用户。LLM 仍会不时生成训练不完整的解决方案，但 RAG 技术有助于改善用户体验。

https://baoyu.io/translations/rag/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation

https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/

https://www.gptsecurity.info/2024/05/26/RAG/
# Prompt Engineering Guide


https://www.promptingguide.ai/zh

# Bellman optimality equations 


# RAG
https://luxiangdong.com/2023/09/25/ragone/

# graphRAG
https://lanesky.github.io/2024/07/27/graph/




# 利用键盘选定文本的快捷键
Shift +↑ 向上选定一行
Shift +↓ 向下选定一行
Shift + ← 向左选定1个字符
Shift + → 向右选定1个字符
Shift + Ctrl +↑ 选定内容扩展至段落首
Shift + Crtl +↓ 选定内容扩展至段落尾
Shift + Crtl + ← 选定内容扩展至单词首
Shift + Crtl + → 选定内容扩展至单词尾
Shift + Home 选定内容扩展至行首
Shift + End 选定内容扩展至行尾
Shift + Ctrl + Home 选定内容扩展文档首
Shift + Crtl + End 选定内容扩展至文档尾
Crtl + A 选定整个文档





#  Variational Autoencoders (VAEs)

two major families stand out and deserve a special attention: Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).

https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73  
https://bluefisher.github.io/2020/02/07/%E7%90%86%E8%A7%A3-Variational-Autoencoders-VAEs/

重建

![alt text](image.png)



# 正则化


# PCA




# Trees, Forests, Bagging, and Boosting


## XGBosting

git add -A
git commit -m ""