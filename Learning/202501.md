# Retrieval Augmented Generation (RAG)


# Fine-tuned model


# zero-shot, one-shot, Few-shot
样本学习
句子：。。。。
情感 good or not

one-shot 给一个样本


# Next.js, Remix


# Machine learning 

# Data pipeline

# ETL


# Typescript, Python, Rust and React, Express, PostgreSQL 





# Create dashboards in Databricks, Power BI, and Bazefield.


# Model Architecture


![alt text](image-4.png)


encoder only: translate token to a semanticaly meaningful representation: tasks: text classification

Decoder only: similar to encoder but does not allow self-attention with future elements: tasks: text generation

encoder-decoder : combines both and allows cross-attention tasks: translation


# 20 tokens per model parameter



# Determine Model Size


The model size in parameters (e.g., 7B, 13B, 33B) indicates how many weights (or parameters) the model contains. Each parameter is typically stored in a specific numerical precision, which determines how much memory is needed.

1 billion (1B) parameters = 1 × 10⁹ parameters.
Typical precision:
FP32 (32-bit): 4 bytes per parameter.
FP16 (16-bit): 2 bytes per parameter (most common for large models).
INT8 (8-bit): 1 byte per parameter (used for quantized models).
Example for a 7B Model:


At FP16 precision: 
7
𝐵
×
2
 
bytes
=
14
 
GB
7B×2bytes=14GB.


Quantization to INT8 cuts it in half: 
7
𝐵
×
1
 
byte
=
7
 
GB
7B×1byte=7GB.