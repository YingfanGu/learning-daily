# Retrieval Augmented Generation (RAG)


# Fine-tuned model


# zero-shot, one-shot, Few-shot
æ ·æœ¬å­¦ä¹ 
å¥å­ï¼šã€‚ã€‚ã€‚ã€‚
æƒ…æ„Ÿ good or not

one-shot ç»™ä¸€ä¸ªæ ·æœ¬


# Next.js, Remix


# Machine learning 

# Data pipeline

# ETL


# Typescript, Python, Rust and React, Express, PostgreSQL 





# Create dashboards in Databricks, Power BI, and Bazefield.


# Model Architecture


![alt text](image-4.png)


encoder only: translate token to a semanticaly meaningful representation: tasks: text classification

Decoder only: similar to encoder but does not allow self-attention with future elements: tasks: text generation

encoder-decoder : combines both and allows cross-attention tasks: translation


# 20 tokens per model parameter



# Determine Model Size


The model size in parameters (e.g., 7B, 13B, 33B) indicates how many weights (or parameters) the model contains. Each parameter is typically stored in a specific numerical precision, which determines how much memory is needed.

1 billion (1B) parameters = 1 Ã— 10â¹ parameters.
Typical precision:
FP32 (32-bit): 4 bytes per parameter.
FP16 (16-bit): 2 bytes per parameter (most common for large models).
INT8 (8-bit): 1 byte per parameter (used for quantized models).
Example for a 7B Model:


At FP16 precision: 
7
ğµ
Ã—
2
â€‰
bytes
=
14
â€‰
GB
7BÃ—2bytes=14GB.


Quantization to INT8 cuts it in half: 
7
ğµ
Ã—
1
â€‰
byte
=
7
â€‰
GB
7BÃ—1byte=7GB.